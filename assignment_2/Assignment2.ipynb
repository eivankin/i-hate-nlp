{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvigâ€™s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spellchecker interface"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from typing import NewType\n",
    "\n",
    "Word = NewType(\"Word\", str)\n",
    "Sentence = NewType(\"Sentence\", str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:00:27.275869987Z",
     "start_time": "2024-03-22T18:00:27.255247903Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Spellchecker(ABC):\n",
    "    vocab: set[Word]\n",
    "    edit_distance: int\n",
    "\n",
    "    @abstractmethod\n",
    "    def correct_sentence(self, sentence: Sentence, skip_in_vocab: bool = True) -> Sentence:\n",
    "        return NotImplemented\n",
    "\n",
    "    @staticmethod\n",
    "    def get_words(text) -> list[Word]:\n",
    "        return list(Word(w) for w in re.findall(r\"\\w+\", text.lower()))\n",
    "\n",
    "    @staticmethod\n",
    "    def edits(word: Word) -> set[Word]:\n",
    "        letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(Word(w) for w in deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edit_up_to_distance(self, word: str) -> list[set[Word]]:\n",
    "        results = [{word}]\n",
    "        for _ in range(self.edit_distance):\n",
    "            edits = set()\n",
    "            for word in results[-1]:\n",
    "                edits |= self.edits(word)\n",
    "            results.append(edits)\n",
    "        return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:03:51.385233424Z",
     "start_time": "2024-03-22T18:03:51.328945269Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare data (train/test split)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentences = nltk.sent_tokenize(Path(\"big.txt\").read_text())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:03:53.056364487Z",
     "start_time": "2024-03-22T18:03:52.178084179Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "57635"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:03:53.101618251Z",
     "start_time": "2024-03-22T18:03:53.047296291Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "1258764"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus, test_corpus = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "Path(\"big.train.txt\").write_text(\" \".join(train_corpus))\n",
    "Path(\"big.test.txt\").write_text(\" \".join(test_corpus))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:03:53.102021319Z",
     "start_time": "2024-03-22T18:03:53.092259389Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Norvig's solution (baseline) "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "MoQeEsZvHvvi",
    "ExecuteTime": {
     "end_time": "2024-03-22T18:03:53.783980008Z",
     "start_time": "2024-03-22T18:03:53.771143008Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class SimpleSpellChecker(Spellchecker):\n",
    "    def __init__(self, train_corpus: Path, edit_distance: int = 2) -> None:\n",
    "        self.words = Counter(self.get_words(train_corpus.read_text()))\n",
    "        self.vocab = set(self.words.keys())\n",
    "        self.word_count = sum(self.words.values())\n",
    "        self.edit_distance = edit_distance\n",
    "\n",
    "    def probability(self, word: str) -> float:\n",
    "        return self.words.get(word, 0) / self.word_count\n",
    "\n",
    "    def correct_one_word(self, word: str) -> str:\n",
    "        return max(self.get_candidates(word), key=self.probability)\n",
    "\n",
    "    def get_candidates(self, word: str) -> set[str]:\n",
    "        for option in map(self.get_known_words, self.edit_up_to_distance(word)):\n",
    "            if option:\n",
    "                return option\n",
    "\n",
    "        return {word}\n",
    "\n",
    "    def get_known_words(self, words: Iterable[str]) -> set[str]:\n",
    "        return set(words).intersection(self.vocab)\n",
    "\n",
    "    def correct_sentence(self, sentence: Sentence, skip_in_vocab: bool = True) -> Sentence:\n",
    "        return Sentence(\" \".join(self.correct_one_word(w) for w in self.get_words(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "sp = SimpleSpellChecker(Path(\"big.train.txt\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:03:54.584476023Z",
     "start_time": "2024-03-22T18:03:54.262463852Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "'justifications'"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.correct_one_word(\"justificaitons\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:03:54.887903882Z",
     "start_time": "2024-03-22T18:03:54.663370222Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "'sherlock holmes'"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.correct_sentence(\"Sherlck Halmes\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:03:55.717123539Z",
     "start_time": "2024-03-22T18:03:55.596960798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "'the way bark'"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.correct_sentence(\"the way bark\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:03:56.336438165Z",
     "start_time": "2024-03-22T18:03:56.321175416Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "896252"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.word_count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:03:57.996410923Z",
     "start_time": "2024-03-22T18:03:57.962110819Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### N-gram spellchecker"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "NO_WORD = \"-\"\n",
    "\n",
    "\n",
    "class NGramSpellChecker(Spellchecker):\n",
    "    def __init__(\n",
    "        self, train_corpus: Path, edit_distance: int = 2, ngram_size: int = 2\n",
    "    ) -> None:\n",
    "        words = self.get_words(train_corpus.read_text())\n",
    "        self.vocab = set(words)\n",
    "        self.word_count = len(words)\n",
    "        \n",
    "        self.edit_distance = edit_distance\n",
    "        self.ngram_counts = Counter(self._get_ngrams(words, ngram_size))\n",
    "        self.ngram_count = sum(self.ngram_counts.values())\n",
    "        self.ngram_size = ngram_size\n",
    "        \n",
    "        self.prefix = defaultdict(int)\n",
    "        self.suffix = defaultdict(int)\n",
    "        \n",
    "        for ngram, count in self.ngram_counts.items():\n",
    "            for k in range(1, len(ngram) + 1):\n",
    "                k_gram = ngram[:k]\n",
    "                self.prefix[k_gram] += count\n",
    "                self.suffix[k_gram[::-1]] += count\n",
    "        self.prefix[tuple()] = self.suffix[tuple()] = self.ngram_count\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_ngrams(words: Iterable[str], ngram_size: int, pad: bool = False) -> list[Word]:\n",
    "        if pad:\n",
    "            words = chain([NO_WORD] * (ngram_size - 1), words)\n",
    "        else:\n",
    "            words = words\n",
    "        return nltk.ngrams(words, ngram_size)\n",
    "\n",
    "    def word_probability(self, context_before: tuple[Word, ...], word: Word, context_after: tuple[Word, ...]) -> float:\n",
    "        before_prob = self.prefix[context_before + (word,)] / self.ngram_count\n",
    "        after_prob = self.suffix[context_after[::-1] + (word,)] / self.ngram_count\n",
    "        return before_prob + after_prob\n",
    "\n",
    "    def correct_word(self, context_before: tuple[Word, ...], word: Word, context_after: tuple[Word, ...]) -> Word:\n",
    "        return max(\n",
    "            set().union(*self.edit_up_to_distance(word)), key=lambda w: self.word_probability(context_before, w, context_after)\n",
    "        )\n",
    "\n",
    "    def correct_sentence(self, sentence: Sentence, skip_in_vocab: bool = True) -> Sentence:\n",
    "        words = self.get_words(sentence)\n",
    "        new_words = []\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if skip_in_vocab and word in self.vocab:\n",
    "                new_words.append(word)\n",
    "            else:\n",
    "                context_before = tuple(new_words[-self.ngram_size:])\n",
    "                context_after = tuple(words[i + 1:i + self.ngram_size])\n",
    "                new_words.append(self.correct_word(context_before, word, context_after))\n",
    "\n",
    "        return Sentence(\" \".join(new_words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:36.078758739Z",
     "start_time": "2024-03-22T18:26:36.044856803Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "ngram_sp = NGramSpellChecker(Path(\"big.train.txt\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:37.943580305Z",
     "start_time": "2024-03-22T18:26:36.232504668Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "data": {
      "text/plain": "'sherlock holmes'"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_sp.correct_sentence(\"Sherlck Halmes\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:38.197301189Z",
     "start_time": "2024-03-22T18:26:37.950008158Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "'the war are'"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_sp.correct_sentence(\"the way bark\", skip_in_vocab=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:38.253087792Z",
     "start_time": "2024-03-22T18:26:38.187853740Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "I used the same train set for both spellcheckers (Norvig's dataset) for more fair comparison. I did not assign any weights to edits and did not use something like beam for simplicity, but adding it may lead to improved accuracy.\n",
    "\n",
    "For n-gram spellchecker I used bigrams and unigrams, both looking forward and backward in context. Trying to add trigrams and etc. did not improve the performance of the model. I decided to add context after word to improve the accuracy, but it did not seem to help as well. Also, I use already predicted words as the context before word because it yields better results. I tried to predict words even if they are in dictionary, but it leads to bad results, maybe adding weights to edits would help with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define test set generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def mutate_word(word: str, edit_probability: float = 1e-1) -> str:\n",
    "    \"\"\"\n",
    "    1 edit probability - 1/10\n",
    "    2 edit probability - 1/100\n",
    "    and etc.\n",
    "    \"\"\"\n",
    "    while random.random() <= edit_probability:\n",
    "        word = next(iter(Spellchecker.edits(word)))\n",
    "    return word"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:13.376733910Z",
     "start_time": "2024-03-22T18:26:13.332439429Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "test_sentences = nltk.sent_tokenize(Path(\"big.test.txt\").read_text())\n",
    "test_words = [Spellchecker.get_words(s) for s in test_sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:13.814521216Z",
     "start_time": "2024-03-22T18:26:13.534981761Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "mutated_words = [[mutate_word(w) for w in s] for s in test_words]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:14.970450345Z",
     "start_time": "2024-03-22T18:26:13.871244850Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "['by',\n 'bmeans',\n 'of',\n 'two',\n 'recurved',\n 'fangs',\n 'attacned',\n 'so',\n 'the',\n 'umper',\n 'jaw',\n 'and',\n 'connected',\n 'by',\n 'a',\n 'ductv',\n 'with',\n 'poison',\n 'secreting',\n 'glands',\n 'they',\n 'intruduce',\n 'into',\n 'their',\n 'prey',\n 'a',\n 'thick',\n 'transparent',\n 'yellowish',\n 'fluid',\n 'of',\n 'acid',\n 'reaction',\n 'probably',\n 'ouf',\n 'the',\n 'wature',\n 'of',\n 'an',\n 'albumose',\n 'and',\n 'known',\n 'as',\n 'the',\n '_venom_']"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutated_words[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:15.012688320Z",
     "start_time": "2024-03-22T18:26:15.004056475Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing solutions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "class DoNothing(Spellchecker):\n",
    "    def correct_sentence(self, sentence: str) -> str:\n",
    "        return sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:15.013110304Z",
     "start_time": "2024-03-22T18:26:15.004262632Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "OwZWaX9VVs7B",
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:15.013316682Z",
     "start_time": "2024-03-22T18:26:15.004318287Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def test_spellchecker(sp: Spellchecker, test_set: list[list[str]] = test_words) -> None:\n",
    "    prepared_data = [\" \".join(s) for s in test_set]\n",
    "    start = time.time()\n",
    "    all_predictions = []\n",
    "    for mutated_sentence in prepared_data:\n",
    "        predicted = sp.correct_sentence(mutated_sentence)\n",
    "        all_predictions.append(predicted)\n",
    "    end = time.time()\n",
    "    all_predictions = [s.split() for s in all_predictions]\n",
    "    elapsed = end - start\n",
    "    print(f\"Time taken: {elapsed:.3f} s\")\n",
    "    total_words = sum(len(s) for s in all_predictions)\n",
    "    print(\"Processed\", total_words, \"words\")\n",
    "    correct_words = sum(\n",
    "        w1 == w2\n",
    "        for s1, s2 in zip(test_words, all_predictions)\n",
    "        for w1, w2 in zip(s1, s2)\n",
    "    )\n",
    "    accuracy = correct_words / total_words\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Rate: {total_words / elapsed:.2f} words/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "NUM_SENTENCES = 20"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:15.651980749Z",
     "start_time": "2024-03-22T18:26:15.636300545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.000 s\n",
      "Processed 431 words\n",
      "Accuracy: 87.01%\n",
      "Rate: 36892755.59 words/s\n"
     ]
    }
   ],
   "source": [
    "test_spellchecker(DoNothing(), mutated_words[:NUM_SENTENCES])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:15.864107156Z",
     "start_time": "2024-03-22T18:26:15.843268566Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 11.442 s\n",
      "Processed 431 words\n",
      "Accuracy: 93.50%\n",
      "Rate: 37.67 words/s\n"
     ]
    }
   ],
   "source": [
    "test_spellchecker(sp, mutated_words[:NUM_SENTENCES])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:27.543453877Z",
     "start_time": "2024-03-22T18:26:16.101426499Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 4.772 s\n",
      "Processed 431 words\n",
      "Accuracy: 90.02%\n",
      "Rate: 90.31 words/s\n"
     ]
    }
   ],
   "source": [
    "test_spellchecker(ngram_sp, mutated_words[:NUM_SENTENCES])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:26:47.631165569Z",
     "start_time": "2024-03-22T18:26:42.859500912Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems that my implementation perform noticeably worse than Norvig's on such test set. It would be nice to test it on some other dataset, but I cannot find any.\n",
    "But at least it is significantly faster than Norvig's\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
